{"cells":[{"cell_type":"markdown","metadata":{"id":"i5Jm3UN_Hfsu"},"source":["## **Homework 3**\n","\n","\n","2022320009 이수현\n","\n","이미지는 픽사베이의 무료 이미지를 사용하였습니다\n","\n","\n","**Instructions**\n","* This homework focuses on understanding and applying DETR for object detection and attention visualization. It consists of **three questions** designed to assess both theoretical understanding and practical application.\n","\n","* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n","\n","* **Deadline: 11/14 (Thur) 23:59**\n","\n","**Reference**\n","* End-to-End Object Detection with Transformers (DETR): https://github.com/facebookresearch/detr"]},{"cell_type":"markdown","metadata":{"id":"G3n9blo4JO7m"},"source":["### **Q1.  Understanding DETR model**\n","\n","* Fill-in-the-blank exercise to test your understanding of critical parts of the DETR model workflow.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SONlVIhPH_qF"},"outputs":[],"source":["from torch import nn\n","class DETR(nn.Module):\n","    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n","                 num_encoder_layers=6, num_decoder_layers=6, num_queries=100):\n","        super().__init__()\n","\n","        # create ResNet-50 backbone\n","        self.backbone = resnet50()\n","        del self.backbone.fc\n","\n","        # create conversion layer\n","        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n","\n","        # create a default PyTorch transformer\n","        self.transformer = nn.Transformer(\n","            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n","\n","        # prediction heads, one extra class for predicting non-empty slots\n","        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n","        self.linear_class = nn.Linear(hidden_dim, num_classes + 1) #모델이 예측해야하는 총 class수. objet가 없는 경우를 위해 +1\n","        self.linear_bbox = nn.Linear(hidden_dim, 4)\n","\n","        # output positional encodings (object queries)\n","        self.query_pos = nn.Parameter(torch.rand(num_queries, hidden_dim)) #\n","\n","        # spatial positional encodings\n","        # note that in baseline DETR we use sine positional encodings\n","        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n","        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n","\n","    def forward(self, inputs):\n","        # propagate inputs through ResNet-50 up to avg-pool layer\n","        x = self.backbone.conv1(inputs)\n","        x = self.backbone.bn1(x)\n","        x = self.backbone.relu(x)\n","        x = self.backbone.maxpool(x)\n","\n","        x = self.backbone.layer1(x)\n","        x = self.backbone.layer2(x)\n","        x = self.backbone.layer3(x)\n","        x = self.backbone.layer4(x)\n","\n","        # convert from 2048 to 256 feature planes for the transformer\n","        h = self.conv(x)\n","\n","        # construct positional encodings\n","        H, W = h.shape[-2:]\n","        pos = torch.cat([\n","            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n","            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n","        ], dim=-1).flatten(0, 1).unsqueeze(1)\n","\n","        # propagate through the transformer\n","        h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1),\n","                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n","\n","\n","\n","        # finally project transformer outputs to class labels and bounding boxes\n","        pred_logits = _______\n","        pred_boxes = _______\n","\n","        return {'pred_logits': pred_logits,\n","                'pred_boxes': pred_boxes}"]},{"cell_type":"markdown","source":["### **takeout messages**\n","\n","[DETR]\n","\n","* anchor box 없애고 단순한 구조\n","    -> 모델이 더 가벼워지고 빠름\n","* end to end 구조\n","    * 기존에는 anchor box -> 후처리(NMS)가 필요\n","    * end to end 구조로 더 단순해짐\n","* transformer 기반 구조\n","    * self attention과 cross attention(encoder-decoder attention)을 이용하여 전반적인 관계와 각 객체간의 연관성 학습\n","* binary matching에 헝가리안 알고리즘을 사용\n","\n","* __init__ 함수\n","    * 주요 구조 : 백본 / transformer encoder-decoder / 예측헤드\n","    * ResNet-50 backbone으로 feature extraction\n","    * CNN으로 feature를 변환 : self.conv = nn.Conv2d(2048, hidden_dim, 1)\n","    * transformer : 인코더와 디코더로 구성\n","    * 예측 헤드 : class 예측, BBOX 예측 -> 선형 계층\n","        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n","        self.linear_bbox = nn.Linear(hidden_dim, 4)\n","\n","* forward\n","    * 백본으로 ResNet-50을 통해 이미지에서 특징 추출\n","    * 특징맵 -> CNN -> transformer input : h = self.conv(x) : 2048차원에서 -> 256차원으로 변환\n","    * positional encoding 구성\n","    * transformer\n","    * 출력 예측 (BBOX, class)"],"metadata":{"id":"jkb580H6R2kt"}},{"cell_type":"markdown","metadata":{"id":"2CGZlqo-HtRN"},"source":["### **Q2. Custom Image Detection and Attention Visualization**\n","\n","In this task, you will upload an **image of your choice** (different from the provided sample) and follow the steps below:\n","\n","* Object Detection using DETR\n"," * Use the DETR model to detect objects in your uploaded image.\n","\n","* Attention Visualization in Encoder\n"," * Visualize the regions of the image where the encoder focuses the most.\n","\n","* Decoder Query Attention in Decoder\n"," * Visualize how the decoder’s query attends to specific areas corresponding to the detected objects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExsO0XgWHsvP"},"outputs":[],"source":["import math\n","\n","from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format = 'retina'\n","\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","import torch\n","from torch import nn\n","\n","\n","from torchvision.models import resnet50\n","import torchvision.transforms as T\n","torch.set_grad_enabled(False);\n","\n","# COCO classes\n","CLASSES = [\n","    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n","    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n","    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n","    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n","    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n","    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n","    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n","    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n","    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n","    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n","    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n","    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n","    'toothbrush'\n","]\n","\n","# colors for visualization\n","COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n","          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n","# standard PyTorch mean-std input image normalization\n","transform = T.Compose([\n","    T.Resize(800),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# for output bounding box post-processing\n","def box_cxcywh_to_xyxy(x):\n","    x_c, y_c, w, h = x.unbind(1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","    return torch.stack(b, dim=1)\n","\n","def rescale_bboxes(out_bbox, size):\n","    img_w, img_h = size\n","    b = box_cxcywh_to_xyxy(out_bbox)\n","    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n","    return b\n","\n","def plot_results(pil_img, prob, boxes):\n","    plt.figure(figsize=(16,10))\n","    plt.imshow(pil_img)\n","    ax = plt.gca()\n","    colors = COLORS * 100\n","    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n","        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n","                                   fill=False, color=c, linewidth=3))\n","        cl = p.argmax()\n","        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n","        ax.text(xmin, ymin, text, fontsize=15,\n","                bbox=dict(facecolor='yellow', alpha=0.5))\n","    plt.axis('off')\n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6TSPybYnIwSE"},"source":["\n","In this section, we show-case how to load a model from hub, run it on a custom image, and print the result.\n","Here we load the simplest model (DETR-R50) for fast inference. You can swap it with any other model from the model zoo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aX2QNanH9T0"},"outputs":[],"source":["model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n","model.eval();\n","\n","#원래 코드 url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n","url = 'https://media.istockphoto.com/id/2148372432/ko/%EC%82%AC%EC%A7%84/%EA%B1%B0%EC%8B%A4-%EC%86%8C%ED%8C%8C%EC%97%90%EC%84%9C-%EC%98%81%ED%99%94%EB%A5%BC-%EB%B3%B4%EA%B3%A0-%EC%9E%88%EB%8A%94-%EA%B0%80%EC%A1%B1%EC%9D%98-%EB%92%B7%EB%AA%A8%EC%8A%B5.jpg?s=1024x1024&w=is&k=20&c=JuETFlcC7Z06peL0zEkiF2rDpWLbuIOZDuMF_jAPBH4='\n","#이미지 출처는 픽사베이(무료이미지)\n","im = Image.open(requests.get(url, stream=True).raw) # put your own image\n","\n","# mean-std normalize the input image (batch-size: 1)\n","img = transform(im).unsqueeze(0)\n","\n","# propagate through the model\n","outputs = model(img)\n","\n","# keep only predictions with 0.7+ confidence\n","probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n","keep = probas.max(-1).values > 0.9\n","\n","# convert boxes from [0; 1] to image scales\n","bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n","\n","# mean-std normalize the input image (batch-size: 1)\n","img = transform(im).unsqueeze(0)\n","\n","# propagate through the model\n","outputs = model(img)\n","\n","# keep only predictions with 0.7+ confidence\n","probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n","keep = probas.max(-1).values > 0.9\n","\n","# convert boxes from [0; 1] to image scales\n","bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n","\n","# mean-std normalize the input image (batch-size: 1)\n","img = transform(im).unsqueeze(0)\n","\n","# propagate through the model\n","outputs = model(img)\n","\n","# keep only predictions with 0.7+ confidence\n","probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n","keep = probas.max(-1).values > 0.9\n","\n","# convert boxes from [0; 1] to image scales\n","bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n","\n","plot_results(im, probas[keep], bboxes_scaled)"]},{"cell_type":"markdown","metadata":{"id":"Z4alpH62I5GS"},"source":["\n","Here we visualize attention weights of the last decoder layer. This corresponds to visualizing, for each detected objects, which part of the image the model was looking at to predict this specific bounding box and class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCNmOSjGH9WV"},"outputs":[],"source":["# use lists to store the outputs via up-values\n","conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n","\n","hooks = [\n","    model.backbone[-2].register_forward_hook(\n","        lambda self, input, output: conv_features.append(output)\n","    ),\n","    model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n","        lambda self, input, output: enc_attn_weights.append(output[1])\n","    ),\n","    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n","        lambda self, input, output: dec_attn_weights.append(output[1])\n","    ),\n","]\n","\n","# propagate through the model\n","outputs = model(img) # put your own image\n","\n","for hook in hooks:\n","    hook.remove() #inference 이후에는 hook 제거. 불필요한 연산 제거\n","\n","# don't need the list anymore\n","conv_features = conv_features[0]\n","enc_attn_weights = enc_attn_weights[0]\n","dec_attn_weights = dec_attn_weights[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrrWPnotI_G-"},"outputs":[],"source":["# get the feature map shape\n","h, w = conv_features['0'].tensors.shape[-2:]\n","\n","fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\n","colors = COLORS * 100\n","for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n","    ax = ax_i[0]\n","    ax.imshow(dec_attn_weights[0, idx].view(h, w))\n","    ax.axis('off')\n","    ax.set_title(f'query id: {idx.item()}')\n","    ax = ax_i[1]\n","    ax.imshow(im)\n","    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n","                               fill=False, color='blue', linewidth=3))\n","    ax.axis('off')\n","    ax.set_title(CLASSES[probas[idx].argmax()])\n","fig.tight_layout()"]},{"cell_type":"markdown","source":["**디코더 attention**\n","1. self attention\n","    \n","    * 디코더가 객체 쿼리들 사이의 상호작용을 학습\n","    * 객체 쿼리들이 서로 독립적으로 작용하지 않음\n","    * 각 쿼리가 예측하려는 객체가 중복되지 않음\n","\n","2. encoder - decoder attention\n","\n","    * 디코더의 객체 쿼리가 인코더가 출력한 특징 맵과 상호작용\n","    * 디코더의 각 쿼리는 인코더의 특징 맵에서 자신이 집중해야 하는 부분에 대한 정보를 출력\n","    * 이 과정에서 디코더가 특정 개체에 집중하여 BBOX와 class를 예측\n","\n","---\n","---\n","\n","**디코더 attention 시각화 분석**\n","* 위의 보라색 이미지\n","\n","    * 각각의 쿼리 ID의 디코더 attention 가중치를 시각화\n","    * 밝은 부분(노란색)이 디코더가 해당 객체에 집중하고 있는 부분\n","    \n","    * id 25는 person에 집중하고 있는데 위 아래 사진을 비교해보면 위의 사진에서 밝은 부분이 사람\n","\n","* 아래의 이미지\n","\n","    * 디코더의 예측결과로, 각 쿼리 ID마다 특정 개체에 집중하여 BBOX와 class를 출력\n","    * person, potted plant, tv, clock, remote, couch을 분류\n","\n","    ---\n","\n","* 특정 객체 쿼리에 대해 이미지 내에서 어떤 영역에 집중하는지를 나타낸다\n","* 디코더의 self attention으로 각각의 개체가 중복되지 않고 탐지\n","* 각 쿼리 id에 따라 디코더는 이미지에서 특정 객체에 집중하며 각 객체에 파란색 BBOX와 class를 분류\n","* 각 id별로 위의 보라색 이미지와 아래의 이미지를 매칭해보면, 각 id의 객체를 탐지하는데 노란색 부분이 가장 큰 도움을 주었다고 해석할 수 있다.\n","* id 25를 예로 들면 노란색 부분을 보고 'person'이라고 판단했다고 이해 (다른 부분도 봤지만 노란색 부분이 가장 많은 도움이 되었다 정도)"],"metadata":{"id":"fxQDO2tZfLca"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBaCqpWcJBRz"},"outputs":[],"source":["# output of the CNN\n","f_map = conv_features['0']\n","print(\"Encoder attention:      \", enc_attn_weights[0].shape) # => 인코더의 self attention 가중치! => 얘를 통해 인코더가 입력 이미지에서 어디에 집중하는지 알 수 있따\n","print(\"Feature map:            \", f_map.tensors.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"th7dtFsDHsyh"},"outputs":[],"source":["# get the HxW shape of the feature maps of the CNN\n","shape = f_map.tensors.shape[-2:]\n","# and reshape the self-attention to a more interpretable shape\n","sattn = enc_attn_weights[0].reshape(shape + shape)\n","print(\"Reshaped self-attention:\", sattn.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eRkeRezH8fX"},"outputs":[],"source":["# downsampling factor for the CNN, is 32 for DETR and 16 for DETR DC5\n","fact = 32 #다운샘플링 feature map 크기가 32배 작아짐\n","\n","# let's select 4 reference points for visualization\n","idxs = [(200, 200), (280, 400), (200, 600), (440, 800),] # -> 인코더의 self attention이 집중하는 영역을 시각화함\n","\n","# here we create the canvas\n","fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n","# and we add one plot per reference point\n","gs = fig.add_gridspec(2, 4)\n","axs = [\n","    fig.add_subplot(gs[0, 0]),\n","    fig.add_subplot(gs[1, 0]),\n","    fig.add_subplot(gs[0, -1]),\n","    fig.add_subplot(gs[1, -1]),\n","]\n","\n","# for each one of the reference points, let's plot the self-attention\n","# for that point\n","# 인코더 셀프 어텐션 시각화\n","for idx_o, ax in zip(idxs, axs):\n","    idx = (idx_o[0] // fact, idx_o[1] // fact)\n","    ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n","    ax.axis('off')\n","    ax.set_title(f'self-attention{idx_o}')\n","\n","# and now let's add the central image, with the reference points as red circles\n","fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n","fcenter_ax.imshow(im)\n","for (y, x) in idxs:\n","    scale = im.height / img.shape[-2]\n","    x = ((x // fact) + 0.5) * fact\n","    y = ((y // fact) + 0.5) * fact\n","    fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n","    fcenter_ax.axis('off')"]},{"cell_type":"markdown","source":["**인코더 attention**\n","1. self attention\n","\n","    * 디코더가 객체 쿼리들 사이의 상호작용을 학습\n","    * 객체 쿼리들이 서로 독립적으로 작용하지 않음\n","    * 각 쿼리가 예측하려는 객체가 중복되지 않음\n","\n","---\n","---\n","\n","**인코더의 self attention 분석**\n","\n","* fact = 32 : 실제 이미지보다 32배 작아짐\n","* 좌우의 파랑노랑 사진들 : 참조포인트에 대한 self attention 가중치를 시각화\n","\n","    * 노랑부분에 attention이 강하게 작용\n","    * 인코더가 해당 참조 포인트가 특정 이미지 영역과 관련 있다고 판단하는 부분\n","* 중간 이미지\n","    * 빨간색 원 : 참조 포인트\n","    * 각 참조포인트에 대해 self attention 가중치가 계산됨\n","\n","    * (200,600)의 경우 원이 tv 근처에 위치해서 왼쪽 위의 self attention(200,600)에서 tv가 있는 곳이 노란색으로 표시됨\n","* 인코더는 이미지의 전반적인 문맥을 이해\n","* 각 참조포인트의 노란색 부분은 인코더가 해당 위치와 관련된 특징을 학습했다고 이해\n","* 즉, (280,400)의 경우 사람의 머리, 주변 환경등 참조포인트의 위치에 대한 특징을 학습함"],"metadata":{"id":"_1nspfPScjl8"}},{"cell_type":"markdown","metadata":{"id":"_Bcx4iX5H1od"},"source":["### **Q3. Understanding Attention Mechanisms**\n","\n","In this task, you focus on understanding the attention mechanisms present in the encoder and decoder of DETR.\n","\n","* Briefly describe the types of attention used in the encoder and decoder, and explain the key differences between them.\n","\n","* Based on the visualized results from Q2, provide an analysis of the distinct characteristics of each attention mechanism in the encoder and decoder. Feel free to express your insights."]},{"cell_type":"markdown","source":["1. attention mechanisms of encoder\n","\n","    * 입력 이미지의 feature map을 처리할 때 self attention을 이용\n","    * 입력 sequence의 각 요소가 다른 모든 요소와의 관계를 학습하여 전반적인 정보를 파악\n","    * 즉, 인코더는 이런 매커니즘을 통해 이미지 전체의 특징을 종합적으로 학습할 수 있다.\n","\n","2. attention mechanisms of decoder\n","\n","    * self attention\n","\n","        * 디코더가 객체 쿼리들 사이의 상호작용을 학습\n","        * 객체 쿼리들이 서로 독립적으로 작용하지 않음\n","        * 각 쿼리가 예측하려는 객체가 중복되지 않음\n","    \n","    * encoder - decoder attention\n","\n","        * 디코더의 객체 쿼리가 인코더가 출력한 특징 맵과 상호작용\n","        * 디코더의 각 쿼리는 인코더의 특징 맵에서 자신이 집중해야 하는 부분에 대한 정보를 출력\n","        * 이 과정에서 디코더가 특정 개체에 집중하여 BBOX와 class를 예측\n","\n","3. encoder와 decoder의 attention 차이점\n","\n","    * 인코더는 self attention만 사용하고 디코더는 slef attention과 encoder-decoder attention을 사용\n","    * 인코더는 self attention만 사용 -> 전체 입력 이미지의 특징을 학습. 이미지의 특징을 요약하고 변환\n","    * 디코더의 self attention -> 쿼리 사이의 관계를 학습. 서로의 관계를 이해하여 객체의 중복이 없음\n","    * 디코더의 encoder-decoder attention -> 디코더의 쿼리가 인코더의 특징맵에서 특정 객체와 관련된 정보를 추출\n","    * 인코더의 목적은 전반적인 특징을 학습하고 요약하여 디코더에 전달하는 것이고\n","    * 디코더는 인코더에게 받은 특징을 기반으로 object detection하여 class와 BBOX 예측\n","\n","4. Based on the visualized results from Q2, provide an analysis of the distinct characteristics of each attention mechanism in the encoder and decoder\n","    \n","    -> 각 시각화 결과 아래에 분석했습니다\n"],"metadata":{"id":"lMVA7TISXKF1"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}