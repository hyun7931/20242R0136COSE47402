{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Jm3UN_Hfsu"
      },
      "source": [
        "## **Homework 4**\n",
        "\n",
        "2022320009 이수현\n",
        "\n",
        "\n",
        "**Instructions**\n",
        "* This homework focuses on understanding and applying CoCoOp for CLIP prompt tuning. It consists of **four questions** designed to assess both theoretical understanding and practical application.\n",
        "\n",
        "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
        "\n",
        "* **Deadline: 11/26 (Sat) 23:59**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeRABv42Ku4E"
      },
      "source": [
        "### **Preparation**\n",
        "\n",
        "* Run the code below before proceeding with the homework.\n",
        "* If an error occurs, click ‘Run Session Again’ and then restart the runtime from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNOsgBEzKucv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mlvlab/ProMetaR.git\n",
        "%cd ProMetaR/\n",
        "\n",
        "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
        "%cd Dassl.pytorch/\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "!cp -r dassl ../\n",
        "# Install this library (no need to re-build if the source code is modified)\n",
        "# !python setup.py develop\n",
        "%cd ..\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "%mkdir outputs\n",
        "%mkdir data\n",
        "\n",
        "%cd data\n",
        "%mkdir eurosat\n",
        "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip -O EuroSAT.zip\n",
        "\n",
        "!unzip -o EuroSAT.zip -d eurosat/\n",
        "%cd eurosat\n",
        "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
        "\n",
        "%cd ../../\n",
        "\n",
        "import os.path as osp\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import argparse\n",
        "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
        "from dassl.config import get_cfg_default\n",
        "from dassl.engine import build_trainer\n",
        "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
        "from dassl.metrics import compute_accuracy\n",
        "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
        "from dassl.optim import build_optimizer, build_lr_scheduler\n",
        "\n",
        "# custom\n",
        "import datasets.oxford_pets\n",
        "import datasets.oxford_flowers\n",
        "import datasets.fgvc_aircraft\n",
        "import datasets.dtd\n",
        "import datasets.eurosat\n",
        "import datasets.stanford_cars\n",
        "import datasets.food101\n",
        "import datasets.sun397\n",
        "import datasets.caltech101\n",
        "import datasets.ucf101\n",
        "import datasets.imagenet\n",
        "import datasets.imagenet_sketch\n",
        "import datasets.imagenetv2\n",
        "import datasets.imagenet_a\n",
        "import datasets.imagenet_r\n",
        "\n",
        "def print_args(args, cfg):\n",
        "    print(\"***************\")\n",
        "    print(\"** Arguments **\")\n",
        "    print(\"***************\")\n",
        "    optkeys = list(args.__dict__.keys())\n",
        "    optkeys.sort()\n",
        "    for key in optkeys:\n",
        "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
        "    print(\"************\")\n",
        "    print(\"** Config **\")\n",
        "    print(\"************\")\n",
        "    print(cfg)\n",
        "\n",
        "def reset_cfg(cfg, args):\n",
        "    if args.root:\n",
        "        cfg.DATASET.ROOT = args.root\n",
        "    if args.output_dir:\n",
        "        cfg.OUTPUT_DIR = args.output_dir\n",
        "    if args.seed:\n",
        "        cfg.SEED = args.seed\n",
        "    if args.trainer:\n",
        "        cfg.TRAINER.NAME = args.trainer\n",
        "    cfg.DATASET.NUM_SHOTS = 16\n",
        "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
        "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
        "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
        "\n",
        "def extend_cfg(cfg):\n",
        "    \"\"\"\n",
        "    Add new config variables.\n",
        "    \"\"\"\n",
        "    from yacs.config import CfgNode as CN\n",
        "    cfg.TRAINER.COOP = CN()\n",
        "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
        "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
        "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
        "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
        "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
        "    cfg.TRAINER.COCOOP = CN()\n",
        "    cfg.TRAINER.COCOOP.N_CTX = 4  # number of context vectors\n",
        "    cfg.TRAINER.COCOOP.CTX_INIT = \"a photo of a\"  # initialization words\n",
        "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
        "    cfg.TRAINER.PROMETAR = CN()\n",
        "    cfg.TRAINER.PROMETAR.N_CTX_VISION = 4  # number of context vectors at the vision branch\n",
        "    cfg.TRAINER.PROMETAR.N_CTX_TEXT = 4  # number of context vectors at the language branch\n",
        "    cfg.TRAINER.PROMETAR.CTX_INIT = \"a photo of a\"  # initialization words\n",
        "    cfg.TRAINER.PROMETAR.PREC = \"fp16\"  # fp16, fp32, amp\n",
        "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_VISION = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
        "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_TEXT = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
        "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
        "    cfg.TRAINER.PROMETAR.ADAPT_LR = 0.0005\n",
        "    cfg.TRAINER.PROMETAR.LR_RATIO = 0.0005\n",
        "    cfg.TRAINER.PROMETAR.FAST_ADAPTATION = False\n",
        "    cfg.TRAINER.PROMETAR.MIXUP_ALPHA = 0.5\n",
        "    cfg.TRAINER.PROMETAR.MIXUP_BETA = 0.5\n",
        "    cfg.TRAINER.PROMETAR.DIM_RATE=8\n",
        "    cfg.OPTIM_VNET = CN()\n",
        "    cfg.OPTIM_VNET.NAME = \"adam\"\n",
        "    cfg.OPTIM_VNET.LR = 0.0003\n",
        "    cfg.OPTIM_VNET.WEIGHT_DECAY = 5e-4\n",
        "    cfg.OPTIM_VNET.MOMENTUM = 0.9\n",
        "    cfg.OPTIM_VNET.SGD_DAMPNING = 0\n",
        "    cfg.OPTIM_VNET.SGD_NESTEROV = False\n",
        "    cfg.OPTIM_VNET.RMSPROP_ALPHA = 0.99\n",
        "    cfg.OPTIM_VNET.ADAM_BETA1 = 0.9\n",
        "    cfg.OPTIM_VNET.ADAM_BETA2 = 0.999\n",
        "    cfg.OPTIM_VNET.STAGED_LR = False\n",
        "    cfg.OPTIM_VNET.NEW_LAYERS = ()\n",
        "    cfg.OPTIM_VNET.BASE_LR_MULT = 0.1\n",
        "    # Learning rate scheduler\n",
        "    cfg.OPTIM_VNET.LR_SCHEDULER = \"single_step\"\n",
        "    # -1 or 0 means the stepsize is equal to max_epoch\n",
        "    cfg.OPTIM_VNET.STEPSIZE = (-1, )\n",
        "    cfg.OPTIM_VNET.GAMMA = 0.1\n",
        "    cfg.OPTIM_VNET.MAX_EPOCH = 10\n",
        "    # Set WARMUP_EPOCH larger than 0 to activate warmup training\n",
        "    cfg.OPTIM_VNET.WARMUP_EPOCH = -1\n",
        "    # Either linear or constant\n",
        "    cfg.OPTIM_VNET.WARMUP_TYPE = \"linear\"\n",
        "    # Constant learning rate when type=constant\n",
        "    cfg.OPTIM_VNET.WARMUP_CONS_LR = 1e-5\n",
        "    # Minimum learning rate when type=linear\n",
        "    cfg.OPTIM_VNET.WARMUP_MIN_LR = 1e-5\n",
        "    # Recount epoch for the next scheduler (last_epoch=-1)\n",
        "    # Otherwise last_epoch=warmup_epoch\n",
        "    cfg.OPTIM_VNET.WARMUP_RECOUNT = True\n",
        "\n",
        "def setup_cfg(args):\n",
        "    cfg = get_cfg_default()\n",
        "    extend_cfg(cfg)\n",
        "    # 1. From the dataset config file\n",
        "    if args.dataset_config_file:\n",
        "        cfg.merge_from_file(args.dataset_config_file)\n",
        "    # 2. From the method config file\n",
        "    if args.config_file:\n",
        "        cfg.merge_from_file(args.config_file)\n",
        "    # 3. From input arguments\n",
        "    reset_cfg(cfg, args)\n",
        "    cfg.freeze()\n",
        "    return cfg\n",
        "\n",
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "def load_clip_to_cpu(cfg): # Load CLIP\n",
        "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
        "    url = clip._MODELS[backbone_name]\n",
        "    model_path = clip._download(url)\n",
        "\n",
        "    try:\n",
        "        # loading JIT archive\n",
        "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
        "        state_dict = None\n",
        "\n",
        "    except RuntimeError:\n",
        "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "    if cfg.TRAINER.NAME == \"\":\n",
        "      design_trainer = \"CoOp\"\n",
        "    else:\n",
        "      design_trainer = cfg.TRAINER.NAME\n",
        "    design_details = {\"trainer\": design_trainer,\n",
        "                      \"vision_depth\": 0,\n",
        "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
        "                      \"language_ctx\": 0}\n",
        "    model = clip.build_model(state_dict or model.state_dict(), design_details)\n",
        "\n",
        "    return model\n",
        "\n",
        "from dassl.config import get_cfg_default\n",
        "cfg = get_cfg_default()\n",
        "cfg.MODEL.BACKBONE.NAME = \"ViT-B/16\" # Set the vision encoder backbone of CLIP to ViT.\n",
        "clip_model = load_clip_to_cpu(cfg)\n",
        "\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model): # 초기화 하는 함수\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts): # 모델 호출\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "@TRAINER_REGISTRY.register(force=True)\n",
        "class CoCoOp(TrainerX):\n",
        "    def check_cfg(self, cfg):\n",
        "        assert cfg.TRAINER.COCOOP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
        "\n",
        "    def build_model(self):\n",
        "        cfg = self.cfg\n",
        "        classnames = self.dm.dataset.classnames\n",
        "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
        "        clip_model = load_clip_to_cpu(cfg)\n",
        "\n",
        "        if cfg.TRAINER.COCOOP.PREC == \"fp32\" or cfg.TRAINER.COCOOP.PREC == \"amp\":\n",
        "            # CLIP's default precision is fp16\n",
        "            clip_model.float()\n",
        "\n",
        "        print(\"Building custom CLIP\")\n",
        "        self.model = CoCoOpCustomCLIP(cfg, classnames, clip_model)\n",
        "\n",
        "        print(\"Turning off gradients in both the image and the text encoder\")\n",
        "        name_to_update = \"prompt_learner\"\n",
        "\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name_to_update not in name:\n",
        "                param.requires_grad_(False)\n",
        "\n",
        "        # Double check\n",
        "        enabled = set()\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                enabled.add(name)\n",
        "        print(f\"Parameters to be updated: {enabled}\")\n",
        "\n",
        "        if cfg.MODEL.INIT_WEIGHTS:\n",
        "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        # NOTE: only give prompt_learner to the optimizer\n",
        "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
        "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
        "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
        "\n",
        "        self.scaler = GradScaler() if cfg.TRAINER.COCOOP.PREC == \"amp\" else None\n",
        "\n",
        "        # Note that multi-gpu training could be slow because CLIP's size is\n",
        "        # big, which slows down the copy operation in DataParallel\n",
        "        device_count = torch.cuda.device_count()\n",
        "        if device_count > 1:\n",
        "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
        "            self.model = nn.DataParallel(self.model)\n",
        "\n",
        "    def before_train(self):\n",
        "        directory = self.cfg.OUTPUT_DIR\n",
        "        if self.cfg.RESUME:\n",
        "            directory = self.cfg.RESUME\n",
        "        self.start_epoch = self.resume_model_if_exist(directory)\n",
        "\n",
        "        # Remember the starting time (for computing the elapsed time)\n",
        "        self.time_start = time.time()\n",
        "\n",
        "\n",
        "    def forward_backward(self, batch):\n",
        "        image, label = self.parse_batch_train(batch)\n",
        "\n",
        "        model = self.model\n",
        "        optim = self.optim\n",
        "        scaler = self.scaler\n",
        "\n",
        "        prec = self.cfg.TRAINER.COCOOP.PREC\n",
        "        loss = model(image, label) # Input image 모델 통과\n",
        "        optim.zero_grad()\n",
        "        loss.backward() # Backward (역전파)\n",
        "        optim.step() # 모델 parameter update\n",
        "\n",
        "        loss_summary = {\"loss\": loss.item()}\n",
        "\n",
        "        if (self.batch_idx + 1) == self.num_batches:\n",
        "            self.update_lr()\n",
        "\n",
        "        return loss_summary\n",
        "\n",
        "    def parse_batch_train(self, batch):\n",
        "        input = batch[\"img\"]\n",
        "        label = batch[\"label\"]\n",
        "        input = input.to(self.device)\n",
        "        label = label.to(self.device)\n",
        "        return input, label\n",
        "\n",
        "    def load_model(self, directory, epoch=None):\n",
        "        if not directory:\n",
        "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
        "            return\n",
        "\n",
        "        names = self.get_model_names()\n",
        "\n",
        "        # By default, the best model is loaded\n",
        "        model_file = \"model-best.pth.tar\"\n",
        "\n",
        "        if epoch is not None:\n",
        "            model_file = \"model.pth.tar-\" + str(epoch)\n",
        "\n",
        "        for name in names:\n",
        "            model_path = osp.join(directory, name, model_file)\n",
        "\n",
        "            if not osp.exists(model_path):\n",
        "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
        "\n",
        "            checkpoint = load_checkpoint(model_path)\n",
        "            state_dict = checkpoint[\"state_dict\"]\n",
        "            epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "            # Ignore fixed token vectors\n",
        "            if \"token_prefix\" in state_dict:\n",
        "                del state_dict[\"token_prefix\"]\n",
        "\n",
        "            if \"token_suffix\" in state_dict:\n",
        "                del state_dict[\"token_suffix\"]\n",
        "\n",
        "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
        "            # set strict=False\n",
        "            self._models[name].load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def after_train(self):\n",
        "      print(\"Finish training\")\n",
        "\n",
        "      do_test = not self.cfg.TEST.NO_TEST\n",
        "      if do_test:\n",
        "          if self.cfg.TEST.FINAL_MODEL == \"best_val\":\n",
        "              print(\"Deploy the model with the best val performance\")\n",
        "              self.load_model(self.output_dir)\n",
        "          else:\n",
        "              print(\"Deploy the last-epoch model\")\n",
        "          acc = self.test()\n",
        "\n",
        "      # Show elapsed time\n",
        "      elapsed = round(time.time() - self.time_start)\n",
        "      elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "      print(f\"Elapsed: {elapsed}\")\n",
        "\n",
        "      # Close writer\n",
        "      self.close_writer()\n",
        "      return acc\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Generic training loops.\"\"\"\n",
        "        self.before_train()\n",
        "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
        "            self.before_epoch()\n",
        "            self.run_epoch()\n",
        "            self.after_epoch()\n",
        "        acc = self.after_train()\n",
        "        return acc\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
        "parser.add_argument(\"--output-dir\", type=str, default=\"outputs/cocoop3\", help=\"output directory\")\n",
        "parser.add_argument(\n",
        "    \"--seed\", type=int, default=1, help=\"only positive value enables a fixed seed\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--config-file\", type=str, default=\"configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\", help=\"path to config file\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--dataset-config-file\",\n",
        "    type=str,\n",
        "    default=\"configs/datasets/eurosat.yaml\",\n",
        "    help=\"path to config file for dataset setup\",\n",
        ")\n",
        "parser.add_argument(\"--trainer\", type=str, default=\"CoOp\", help=\"name of trainer\")\n",
        "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
        "parser.add_argument(\n",
        "    \"--model-dir\",\n",
        "    type=str,\n",
        "    default=\"\",\n",
        "    help=\"load model from this directory for eval-only mode\",\n",
        ")\n",
        "parser.add_argument(\"--train-batch-size\", type=int, default=4)\n",
        "parser.add_argument(\"--epoch\", type=int, default=10)\n",
        "parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
        "parser.add_argument(\n",
        "    \"--load-epoch\", type=int, default=0, help=\"load model weights at this epoch for evaluation\"\n",
        ")\n",
        "args = parser.parse_args([])\n",
        "\n",
        "def main(args):\n",
        "    cfg = setup_cfg(args)\n",
        "    if cfg.SEED >= 0:\n",
        "        set_random_seed(cfg.SEED)\n",
        "\n",
        "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    trainer = build_trainer(cfg)\n",
        "    if args.eval_only:\n",
        "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
        "        acc = trainer.test()\n",
        "        return acc\n",
        "\n",
        "    acc = trainer.train()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3n9blo4JO7m"
      },
      "source": [
        "### **Q1.  Understanding and implementing CoCoOp**\n",
        "* We have learned how to define CoOp in Lab Session 4.\n",
        "\n",
        "* The main difference between CoOp and CoCoOp is **meta network** to extract image tokens that is added to the text prompt.\n",
        "\n",
        "* Based on the CoOp code given in Lab Session 4, fill-in-the-blank exercise (4 blanks!!) to test your understanding of critical parts of the CoCoOp.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SONlVIhPH_qF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CoCoOpPromptLearner(nn.Module):\n",
        "    def __init__(self, cfg, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
        "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        vis_dim = clip_model.visual.output_dim\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
        "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
        "\n",
        "        if ctx_init:\n",
        "            # use given words to initialize context vectors\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            # random initialization\n",
        "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_vectors)  # Wrap the initialized prompts above as parameters to make them trainable.\n",
        "\n",
        "        ### Tokenize ###\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]  # 예) \"Forest\"\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames] # 예) \"A photo of Forest.\"\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # 예) [49406, 320, 1125, 539...]\n",
        "\n",
        "\n",
        "\n",
        "        #####################################\n",
        "        ####### Q1. Fill in the blank #######   #이 부분이 meta netwrok 정의\n",
        "        ########## Define Meta Net ##########\n",
        "        self.meta_net = nn.Sequential(OrderedDict([\n",
        "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)), #Q1 #이미지 faature를 압축. vis_dim : 이미지 특징 벡터의 차원.\n",
        "            (\"relu\", nn.ReLU(inplace=True)),  #relu activation func\n",
        "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim)) #vis_dim -> ctx_dim(:컨텍스트 차원)으로 매핑\n",
        "        ]))\n",
        "        #####################################\n",
        "        ## Hint: meta network is composed to linear layer, relu activation, and linear layer.\n",
        "        #input 이미지의 특징을 컨텍스트 차원으로 매핑. 2개의 linear layer와 relu함수를 사용\n",
        "\n",
        "\n",
        "\n",
        "        if cfg.TRAINER.COCOOP.PREC == \"fp16\":\n",
        "            self.meta_net.half()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
        "        self.name_lens = name_lens\n",
        "\n",
        "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
        "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
        "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
        "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
        "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
        "\n",
        "        if label is not None:\n",
        "            prefix = prefix[label]\n",
        "            suffix = suffix[label]\n",
        "\n",
        "        prompts = torch.cat(\n",
        "            [\n",
        "                prefix,  # (dim0, 1, dim)\n",
        "                ctx,  # (dim0, n_ctx, dim)\n",
        "                suffix,  # (dim0, *, dim)\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        return prompts\n",
        "\n",
        "    def forward(self, im_features): #im_features : input image feature vector. 이미지 인코더의 출력값\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx  # (n_ctx, ctx_dim)\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        ########## Q2,3. Fill in the blank #########\n",
        "        bias = self.meta_net(im_features)#Q2.  Hint: Image feature is given as input to meta network\")  # (batch, ctx_dim)\n",
        "        bias = bias.unsqueeze(1)  # 차원 확장 (batch_size, ctx_dim) -> (batch, 1, ctx_dim)\n",
        "        ctx = ctx.unsqueeze(0)  # 차원 확장 -> (1, n_ctx, ctx_dim)\n",
        "        ctx_shifted = ctx + bias #Q3. \"Fill in here, Hint: Add meta token to context token\"  # (batch, n_ctx, ctx_dim)\n",
        "        ############################################\n",
        "        ############################################\n",
        "        # bias는 이미지 조건부로 생성된 벡터\n",
        "        # ctx는 컨텍스트 벡터. 학습 가능한 초기값\n",
        "        # ctx는 고정된 컨텍스트 벡터로 모든 이미지에 동일하게 적용. CoOp는 이걸로 각 이미지의 특징을 반영하지 못함\n",
        "        # 그래서 나온게 bias\n",
        "        # bias는 meta network가 이미지 특징 벡터를 입력받아 생성한 각 이미지 특징이 반영된 벡터\n",
        "        # CoCoOp는 bias를 ctx에 더해 조정된 ctx를 이용해서 이미지별로 특징을 반영할 수 있음.\n",
        "\n",
        "\n",
        "\n",
        "        # Use instance-conditioned context tokens for all classes\n",
        "        prompts = []\n",
        "        for ctx_shifted_i in ctx_shifted:\n",
        "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
        "            prompts.append(pts_i)\n",
        "        prompts = torch.stack(prompts)\n",
        "\n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_BluKEdKA94"
      },
      "outputs": [],
      "source": [
        "class CoCoOpCustomCLIP(nn.Module):\n",
        "    def __init__(self, cfg, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = CoCoOpPromptLearner(cfg, classnames, clip_model)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, image, label=None):\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "\n",
        "        image_features = self.image_encoder(image.type(self.dtype)) #이미지 특징 벡터 추출\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        ########## Q4. Fill in the blank #########\n",
        "        prompts = self.prompt_learner(image_features) #Q4.이미지특징을 prompt learner로 전달\n",
        "        ############################################\n",
        "        ############################################\n",
        "        #이미지특징을 prompt learner로 전달,\n",
        "        #그럼 이제 prompl_learner는 이미지 특징 벡터를 받아 각 이미지별로, 클래스별로 조정된 텍스트 프롬포트를 return\n",
        "        #prompts크기: (batch_size, n_cls, n_tkn, ctx_dim)\n",
        "\n",
        "        #이미지와 텍스트 특징 매칭\n",
        "        logits = []\n",
        "        for pts_i, imf_i in zip(prompts, image_features):\n",
        "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            l_i = logit_scale * imf_i @ text_features.t()\n",
        "            logits.append(l_i)\n",
        "        logits = torch.stack(logits)\n",
        "\n",
        "        if self.prompt_learner.training:\n",
        "            return F.cross_entropy(logits, label)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**takeout messages**\n",
        "- CoOp : 고정된 text prompt로 모델이 학습하도록 최적화\n",
        "- CoCoOp : 추가적으로 이미지의 특성에 따라 동적인 text context를 생성. meta network를 사용하여 image toekn을 추출하고 추출한 것을 text prompt와 결합\n",
        "---\n",
        "- ctx는 모든 이미지에 동일하게 적용되는 초기 컨텍스트\n",
        "- bias는 meta network가 이미지 특징 벡터를 입력받아 생성한 각 이미지의 특성을 반영한 벡터. 각 이미지마다 다른 값을 가짐\n",
        "---\n",
        "- ctx랑 bias가 더하기에는 차원이 안맞아서 조정해줘야함(unsqueeze)\n",
        "- ctx랑 bias를 알맞게 차원확장해주고\n",
        "- 브로드캐스팅 규칙에 따라 더함 -> shifted_ctx\n",
        "- 여기서 확장할 때 1로 확장한 이유는 1은 상대방의 차원을 그대로 복사해오기 때문\n",
        "---\n",
        "- CoOp는 ctx만 이용해서 각 이미지의 특징을 반영하지 못한다는 단점이 있음\n",
        "- CoCoOp 는 ctx에 bias를 더해 shifted_ctx를 이용함으로써 각 이미지의 특징을 반영할 수 있음.\n",
        "---\n",
        "- 이미지 특징을 promp learner로 전달해서 각 이미지별로, 클래스별로 알맞은 택스트 프롬프트를 return함\n",
        "- 특징 매칭\n",
        "\n",
        "\n",
        "[same in english]\n",
        "- CoOp : Optimizes model to learn with fixed text prompt\n",
        "- CoCoOp: Additionally, a dynamic text context is created based on the characteristics of the image. Image toekn is extracted using the meta network and combined with the text prompt\n",
        "---\n",
        "- ctx is an initial context that applies equally to all images\n",
        "- The bias is a vector that reflects the characteristics of each image generated by the metannetwork by receiving the image feature vector. Each image has a different value\n",
        "---\n",
        "- Ctx and bias are not the right dimensions to add, so they need to be adjusted (unsqueeze)\n",
        "- It expands the size of CTX and Bias\n",
        "- Added according to broadcasting rules -> shifted_ctx\n",
        "- The reason why I expanded to 1 when expanding here is because 1 copies the other person's dimensions\n",
        "---\n",
        "- CoOp has the disadvantage of not reflecting the features of each image using only ctx\n",
        "- CoCoOp can reflect the characteristics of each image by using shifted_ctx by adding bias to ctx.\n",
        "---\n",
        "- Transfer image features to the prompeller to return the appropriate tact prompt for each image and for each class\n",
        "- feature matching\n"
      ],
      "metadata": {
        "id": "1jd4aAXvektJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGZlqo-HtRN"
      },
      "source": [
        "### **Q2. Trainining CoCoOp**\n",
        "\n",
        "In this task, you will train CoCoOp on the EuroSAT dataset. If your implementation of CoCoOp in Question 1 is correct, the following code should execute without errors. Please submit the execution file so we can evaluate whether your code runs without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy3bAMnBMrXP"
      },
      "outputs": [],
      "source": [
        "# Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
        "args.trainer = \"CoCoOp\"\n",
        "args.train_batch_size = 4\n",
        "args.epoch = 100\n",
        "args.output_dir = \"outputs/cocoop\"\n",
        "\n",
        "args.subsample_classes = \"base\"\n",
        "args.eval_only = False\n",
        "cocoop_base_acc = main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xql7WpJ5vPII"
      },
      "outputs": [],
      "source": [
        "# Accuracy on the New Classes.\n",
        "args.model_dir = \"outputs/cocoop\"\n",
        "args.output_dir = \"outputs/cocoop/new_classes\"\n",
        "args.subsample_classes = \"new\"\n",
        "args.load_epoch = 100\n",
        "args.eval_only = True\n",
        "coop_novel_acc = main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1KdgiKFsowj"
      },
      "source": [
        "### **Q3. Analyzing the results of CoCoOp**\n",
        "Compare the results of CoCoOp with those of CoOp that we trained in Lab Session 4. Discuss possible reasons for the performance differences observed between CoCoOp and CoOp."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Base Class 성능비교\n",
        "\n",
        "| Metric         | CoOp           | CoCoOp         |\n",
        "|----------------|----------------|----------------|\n",
        "| Total          | 4,200          | 4,200          |\n",
        "| Correct        | 3,839          | 3,813          |\n",
        "| Accuracy (%)   | 91.4%          | 90.8%          |\n",
        "| Error (%)      | 8.6%           | 9.2%           |\n",
        "| Macro F1 (%)   | 91.5%          | 90.9%          |\n",
        "| Elapsed Time   | 0:03:10        | 0:06:09        |\n",
        "\n",
        "- CoOp가 accuracy와 macro F1을 봤을 때 더 높은 값을 가지고, error는 더 적은 것으로 보아 CoOp의 성능이 더 좋다.\n",
        "- CoOp는 고정된 ctx를 사용하므로 학습 데이터에 더 최적화 되어있다.\n",
        "- 반면 CoCoOp는 ctx에 bias를 더해 shifted_ctx를 사용하여 동적으로 프롬프트를 생성하는데, 이 과정에서 base class에 상대적으로 덜 최적화 되기 때문에 CoOp보다 떨어지는 성능을 보인다고 생각할 수 있다.\n",
        "- Elapsed Time을 보면 CoCoOp가 더 오래 걸리는데, 이는 각 이미지의 특징을 반영하기위해 meta network를 사용하기 때문에 더 오랜 시간이 걸리는 것이다.\n",
        "-반면 CoOp는 고정된 컨텍스트를 사용하여 추가연산과정이 없기 때문에 더 빠르다\n",
        "\n",
        "2. New Class 성능 비교\n",
        "\n",
        "| Metric         | CoOp           | CoCoOp         |\n",
        "|----------------|----------------|----------------|\n",
        "| Total          | 3,900          | 3,900          |\n",
        "| Correct        | 2,007          | 1,687          |\n",
        "| Accuracy (%)   | 51.5%          | 43.3%          |\n",
        "| Error (%)      | 48.5%          | 56.7%          |\n",
        "| Macro F1 (%)   | 45.6%          | 39.0%          |\n",
        "\n",
        "- accuracy나 marco F1을 봤을 때 CoOp가 더 높은 퍼센트를 가지고, error도 CoOp가 더 낮다.\n",
        "- 각 이미지별 특징을 반영할 수 있는 CoCoOp의 성능이 더 높을 것이라는 예상과 다르게 new class에서도 CoOp가 더 좋은 성능을 보인다.\n",
        "- CoCoOp의 동적 텍스트 컨텍스트 생성 과정이 new class에 기대만큼 효과적으로 작동하지는 않았음을 보여준다.\n",
        "- 기대만큼의 성능이 나오지 않은 이유를 다음과 같이 생각해 볼 수 있다.\n",
        "    1. meta network에서 imgae feature를 텍스트 컨텍스트로 매핑할 때, new class의 특징을 잘 반영하지 못했을 가능성\n",
        "    2. 데이터셋에서 base class와 new class에 큰 차이가 있을 경우, 동적 프롬프트 생성방식이 비효율적으로 작용했을 가능성.\n",
        "    3. CoCoOp의 동적 프롬프트 생성이 base class에 과적합되어, new class에 일반화되지 않았을 가능성.\n",
        "\n",
        "3. CoCoOp의 성능을 개선하기 위해 다음과 같은 방안을 생각해볼 수 있다.\n",
        "    - data augmentation으로 데이터셋을 다양하게 확장\n",
        "    - base class와 new class의 데이터간의 차이를 줄이기 위해 더 많고 더 다양한 학습데이터를 확보\n",
        "    - meta network가 너무 많은 정보를 텍스트 컨텍스트 매핑으로 전달하지 않도록(더 일반화하기위해) 기준점이나 bottleneck 등을 도입하는 방식\n",
        "    - base class에 과적합을 방지하기위해 regularization을 강화\n",
        "\n",
        "---\n",
        "\n",
        "[same in english]\n",
        "1. Base Class Performance Comparison\n",
        "\n",
        "    - CoOp's performance is better because CoOp has a higher value and fewer errors when looking at acuracy and macro F1.\n",
        "    - CoOp is more optimized for training data as it uses fixed ctx.\n",
        "    - On the other hand, CoCoOp dynamically generates prompts using shifted_ctx by adding bias to ctx, which can be considered to perform worse than CoOp because it is relatively less optimized for the base class.\n",
        "    - Looking at Elapsed Time, CoCoOp takes longer, which takes longer because the meta network is used to reflect the features of each image.\n",
        "    -CoOp, on the other hand, is faster because there is no additional computation process using a fixed context\n",
        "\n",
        "2. New Class Performance Comparison\n",
        "\n",
        "    - When looking at accuracy or marco F1, CoOp has a higher percentage and error is lower.\n",
        "    - Contrary to the expectation that CoCoOp's performance, which can reflect the characteristics of each image, will be higher, CoOp shows better performance even in the new class.\n",
        "    - We show that the dynamic text context generation process of CoCoOp did not work as effectively as expected for the new class.\n",
        "    - The reason why the performance did not come out as expected can be considered as follows.\n",
        "        1. When mapping the image feature to text context in the meta network, it is likely that it did not reflect the new class's features well\n",
        "        2. If there is a large difference between base class and new class in the dataset, it is possible that the dynamic prompt generation method worked inefficiently.\n",
        "        3. The possibility that the dynamic prompt generation of CoCoOp was overfitting the base class and not generalized to the new class.\n",
        "\n",
        "3. To improve the performance of CoCoOp, we can consider the following.\n",
        "    - Data augmentation scales datasets in a variety of ways\n",
        "    - Get more and more diverse training data to reduce the difference between base class and new class data\n",
        "    - How the meta network introduces a reference point or botleneck, etc., so that it does not pass too much information to the text context mapping (to further generalize)\n",
        "    - Strengthen regularization to prevent overfitting to the base class"
      ],
      "metadata": {
        "id": "GaUIovh9WYLR"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "G3n9blo4JO7m",
        "2CGZlqo-HtRN"
      ],
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}